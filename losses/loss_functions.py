import torch
from torch import Tensor
import torch.nn as nn


def get_gp(real: Tensor, fake: Tensor, disc: nn.Module, alpha: float, gamma: float) -> Tensor:
    """
    Computes the gradient penalty for WGAN-GP.

    Args:
        real (Tensor): Real samples.
        fake (Tensor): Fake samples generated by the generator.
        disc (Module): The critic (discriminator) network.
        alpha (float): Interpolation factor.
        gamma (float): Gradient penalty coefficient.

    Returns:
        Tensor: The gradient penalty term.
    """
    mix_images = real * alpha + fake * (1 - alpha)
    mix_scores = disc(mix_images)

    gradient = torch.autograd.grad(
        inputs=mix_images,
        outputs=mix_scores,
        grad_outputs=torch.ones_like(mix_scores),
        retain_graph=True,
        create_graph=True
    )[0]

    gradient = gradient.view(len(gradient), -1)
    gradient_norm = gradient.norm(2, dim=1)
    gp = gamma * ((gradient_norm - 1) ** 2).mean()
    return gp


def gan_loss(real: Tensor, fake: Tensor, disc: nn.Module, alpha: float, gamma: int or float) -> tuple[Tensor, Tensor]:
    """
    Computes the WGAN-GP discriminator and generator loss.

    Args:
        real (Tensor): Real samples.
        fake (Tensor): Fake samples.
        disc (Module): The critic (discriminator).
        alpha (float): Interpolation factor.
        gamma (float): Gradient penalty coefficient.

    Returns:
        tuple: (discriminator_loss, generator_loss)
    """
    disc_real_pred = disc(real)
    disc_fake_pred = disc(fake)
    gp = get_gp(real, fake.detach(), disc, alpha, gamma)
    disc_loss = disc_fake_pred.mean() - disc_real_pred.mean() + gp
    gen_loss = -disc_fake_pred.mean()
    return disc_loss, gen_loss


def class_loss(logits: Tensor, targets: Tensor) -> Tensor:
    loss_func = nn.CrossEntropyLoss()
    loss = loss_func(logits, targets)
    return loss


def recon_loss(fake: Tensor, real: Tensor, alpha: float) -> Tensor:
    """
    Compute MSE and SAM loss between predicted and target spectra.

    Args:
        fake (Tensor): generated sample
        real (Tensor): real sample
        alpha (float): weighing coefficient for balancing MSE/SAM

    Returns:
        Tensor: MSE + SAM reconstruction loss
    """
    eps = 1e-8

    fake = fake.view(fake.size(0), -1)  # (B, C)
    real = real.view(real.size(0), -1)  # (B, C)

    dot_product = torch.sum(fake * real, dim=1)
    norm_fake = torch.norm(fake, p=2, dim=1)
    norm_real = torch.norm(real, p=2, dim=1)

    cosine = dot_product / (norm_fake * norm_real + eps)
    cosine = torch.clamp(cosine, -1 + eps, 1 - eps)
    angle = torch.acos(cosine)
    sam = angle.mean()

    mse_loss = nn.MSELoss(reduction='mean')
    mse = mse_loss(fake, real)

    return alpha * sam + (1-alpha) * mse


def kl_loss(mu: Tensor, log_s: Tensor) -> Tensor:
    """
    Compute the KL divergence between a given Gaussian N(mu, sigma^2)
    and the standard normal distribution N(0, 1).

    Args:
        mu (Tensor): Mean of the latent Gaussian [batch_size, latent_dim]
        log_s (Tensor): Log-variance of the latent Gaussian [batch_size, latent_dim]

    Returns:
        Tensor: KL divergence (scalar)
    """
    kl = -0.5 * torch.sum(1 + log_s - mu.pow(2) - log_s.exp(), dim=1)  # [batch_size]
    return kl.mean()