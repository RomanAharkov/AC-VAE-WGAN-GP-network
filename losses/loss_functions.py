import torch
from torch import Tensor
import torch.nn as nn


def get_gp(real: Tensor, fake: Tensor, disc: nn.Module, alpha: Tensor, gamma: float, device: str) -> Tensor:
    """
    Computes the gradient penalty for WGAN-GP.

    Args:
        real (Tensor): Real samples.
        fake (Tensor): Fake samples generated by the generator.
        disc (Module): The critic (discriminator) network.
        alpha (Tensor): Interpolation factor.
        gamma (float): Gradient penalty coefficient.

    Returns:
        Tensor: The gradient penalty term.
    """
    mix_images = real * alpha + fake * (1 - alpha)
    mix_images.requires_grad_()
    mix_scores = disc(mix_images)

    gradient = torch.autograd.grad(
        inputs=mix_images,
        outputs=mix_scores,
        grad_outputs=torch.ones_like(mix_scores, device=device),
        create_graph=True
    )[0]

    gp = ((gradient.view(len(gradient), -1).norm(2, dim=1) - 1) ** 2).mean()

    return gp


def disc_loss(real: Tensor, fake: Tensor, disc: nn.Module, gamma: int or float, device: str) -> Tensor:
    """
    Computes the WGAN-GP discriminator loss.
    Returns:
        tensor: discriminator_loss
    """
    alpha = torch.rand(len(real), 1, 1, device=device, requires_grad=True)
    gp = get_gp(real, fake, disc, alpha, gamma, device)

    disc_real_pred = disc(real)
    disc_fake_pred = disc(fake)
    disc_loss = disc_fake_pred.mean() - disc_real_pred.mean() + gamma * gp

    with torch.no_grad():
        print(f"D(real)={disc_real_pred.mean():.3f}  D(fake)={disc_fake_pred.mean():.3f}  GP={gp.item():.3f}")

    return disc_loss


def gen_loss(fake: Tensor, disc: nn.Module) -> Tensor:
    """
        Computes the WGAN-GP generator loss.
        Returns:
            tensor: generator_loss
        """
    disc_fake_pred = disc(fake)
    gen_loss = -disc_fake_pred.mean()

    with torch.no_grad():
        print(f"D(fake) for G={disc_fake_pred.mean():.3f}")

    return gen_loss


def class_loss(logits: Tensor, targets: Tensor) -> Tensor:
    loss_func = nn.CrossEntropyLoss()
    loss = loss_func(logits, targets)
    return loss


def recon_loss(fake: Tensor, real: Tensor, alpha: float) -> Tensor:
    """
    Compute MSE and SAM loss between predicted and target spectra.

    Args:
        fake (Tensor): generated sample
        real (Tensor): real sample
        alpha (float): weighing coefficient for balancing MSE/SAM

    Returns:
        Tensor: MSE + SAM reconstruction loss
    """
    eps = 1e-8

    fake = fake.view(fake.size(0), -1)  # (B, C)
    real = real.view(real.size(0), -1)  # (B, C)

    dot_product = torch.sum(fake * real, dim=1)
    norm_fake = torch.norm(fake, p=2, dim=1)
    norm_real = torch.norm(real, p=2, dim=1)

    cosine = dot_product / (norm_fake * norm_real + eps)
    cosine = torch.clamp(cosine, -1 + eps, 1 - eps)
    angle = torch.acos(cosine)
    sam = angle.mean()

    mse_loss = nn.MSELoss(reduction='mean')
    mse = mse_loss(fake, real)

    return alpha * sam + (1-alpha) * mse


def kl_loss(mu: Tensor, log_s: Tensor) -> Tensor:
    """
    Compute the KL divergence between a given Gaussian N(mu, sigma^2)
    and the standard normal distribution N(0, 1).

    Args:
        mu (Tensor): Mean of the latent Gaussian [batch_size, latent_dim]
        log_s (Tensor): Log-variance of the latent Gaussian [batch_size, latent_dim]

    Returns:
        Tensor: KL divergence (scalar)
    """
    kl = -0.5 * torch.sum(1 + log_s - mu.pow(2) - log_s.exp(), dim=1)  # [batch_size]
    return kl.mean()
